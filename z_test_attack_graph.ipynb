{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/songsh/anaconda3/envs/ALOHA/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Loading dataset Attack-Cora...\n",
      "train_mask, test_mask, val_mask sizes : tensor(2235) tensor(125) tensor(125)\n",
      "[I] Finished loading.\n",
      "[I] Data load time: 1.0589s\n"
     ]
    }
   ],
   "source": [
    "from datasets_dgl.data_dgl import *\n",
    "from  easydict  import EasyDict\n",
    "from datasets_dgl.utils import to_scipy\n",
    "\n",
    "torch.cuda.set_device(0)  # 指第几块GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# pubmed 攻击用的是meta_self所以攻击test多\n",
    "dataset_name = 'Attack-Cora'\n",
    "attack       = 'Meta_Self-0.25'\n",
    "\n",
    "################# no attack\n",
    "DATASET0 = EasyDict()\n",
    "DATASET0.ATTACK = {\n",
    "    \"data\"          :dataset_name,\n",
    "    \"attack\"        :attack.split('-')[0],\n",
    "    \"ptb_rate\"      :attack.split('-')[1],\n",
    "    \"train_size\"    :0.9,\n",
    "    \"val_size\"      :0.05,\n",
    "    \"test_size\"     :0.05,\n",
    "    \"group\"         :9,      # train_size * 10\n",
    "    \"use_g1_split\"  :False\n",
    "}\n",
    "dataset  = load_attack_data(DATASET0['ATTACK'])\n",
    "graph = dataset.graph\n",
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = graph.ndata[\"feat\"]\n",
    "labels = graph.ndata[\"label\"]\n",
    "masks = graph.ndata[\"train_mask\"], graph.ndata[\"val_mask\"], graph.ndata[\"test_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # two-layer GCN\n",
    "        self.layers.append(\n",
    "            dglnn.GraphConv(in_size, hid_size, activation=F.relu)\n",
    "        )\n",
    "        self.layers.append(dglnn.GraphConv(hid_size, out_size))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "def evaluate(g, features, labels, mask, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def train(g, features, labels, masks, model):\n",
    "    # define train/val samples, loss function and optimizer\n",
    "    train_mask = masks[0]\n",
    "    val_mask = masks[1]\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        logits = model(g, features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = evaluate(g, features, labels, val_mask, model)\n",
    "        print(\n",
    "            \"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \".format(\n",
    "                epoch, loss.item(), acc\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 00000 | Loss 1.9477 | Accuracy 0.2720 \n",
      "Epoch 00001 | Loss 1.8362 | Accuracy 0.3680 \n",
      "Epoch 00002 | Loss 1.7498 | Accuracy 0.3840 \n",
      "Epoch 00003 | Loss 1.6621 | Accuracy 0.4240 \n",
      "Epoch 00004 | Loss 1.5761 | Accuracy 0.4320 \n",
      "Epoch 00005 | Loss 1.5038 | Accuracy 0.4720 \n",
      "Epoch 00006 | Loss 1.4351 | Accuracy 0.5440 \n",
      "Epoch 00007 | Loss 1.3470 | Accuracy 0.5760 \n",
      "Epoch 00008 | Loss 1.2884 | Accuracy 0.6240 \n",
      "Epoch 00009 | Loss 1.2050 | Accuracy 0.6800 \n",
      "Epoch 00010 | Loss 1.1292 | Accuracy 0.6880 \n",
      "Epoch 00011 | Loss 1.0637 | Accuracy 0.6960 \n",
      "Epoch 00012 | Loss 0.9977 | Accuracy 0.7040 \n",
      "Epoch 00013 | Loss 0.9574 | Accuracy 0.7200 \n",
      "Epoch 00014 | Loss 0.8835 | Accuracy 0.7280 \n",
      "Epoch 00015 | Loss 0.8502 | Accuracy 0.7280 \n",
      "Epoch 00016 | Loss 0.7830 | Accuracy 0.7360 \n",
      "Epoch 00017 | Loss 0.7410 | Accuracy 0.7360 \n",
      "Epoch 00018 | Loss 0.7085 | Accuracy 0.7360 \n",
      "Epoch 00019 | Loss 0.6672 | Accuracy 0.7360 \n",
      "Epoch 00020 | Loss 0.6026 | Accuracy 0.7360 \n",
      "Epoch 00021 | Loss 0.5743 | Accuracy 0.7360 \n",
      "Epoch 00022 | Loss 0.5255 | Accuracy 0.7360 \n",
      "Epoch 00023 | Loss 0.5340 | Accuracy 0.7360 \n",
      "Epoch 00024 | Loss 0.4858 | Accuracy 0.7360 \n",
      "Epoch 00025 | Loss 0.4871 | Accuracy 0.7360 \n",
      "Epoch 00026 | Loss 0.4531 | Accuracy 0.7360 \n",
      "Epoch 00027 | Loss 0.4193 | Accuracy 0.7360 \n",
      "Epoch 00028 | Loss 0.4297 | Accuracy 0.7360 \n",
      "Epoch 00029 | Loss 0.3848 | Accuracy 0.7360 \n",
      "Epoch 00030 | Loss 0.3819 | Accuracy 0.7360 \n",
      "Epoch 00031 | Loss 0.3709 | Accuracy 0.7360 \n",
      "Epoch 00032 | Loss 0.3757 | Accuracy 0.7360 \n",
      "Epoch 00033 | Loss 0.3404 | Accuracy 0.7360 \n",
      "Epoch 00034 | Loss 0.3345 | Accuracy 0.7360 \n",
      "Epoch 00035 | Loss 0.3222 | Accuracy 0.7360 \n",
      "Epoch 00036 | Loss 0.3198 | Accuracy 0.7360 \n",
      "Epoch 00037 | Loss 0.3033 | Accuracy 0.7360 \n",
      "Epoch 00038 | Loss 0.3022 | Accuracy 0.7360 \n",
      "Epoch 00039 | Loss 0.2897 | Accuracy 0.7360 \n",
      "Epoch 00040 | Loss 0.2752 | Accuracy 0.7360 \n",
      "Epoch 00041 | Loss 0.2762 | Accuracy 0.7360 \n",
      "Epoch 00042 | Loss 0.2880 | Accuracy 0.7360 \n",
      "Epoch 00043 | Loss 0.2635 | Accuracy 0.7360 \n",
      "Epoch 00044 | Loss 0.2700 | Accuracy 0.7360 \n",
      "Epoch 00045 | Loss 0.2588 | Accuracy 0.7360 \n",
      "Epoch 00046 | Loss 0.2514 | Accuracy 0.7360 \n",
      "Epoch 00047 | Loss 0.2650 | Accuracy 0.7440 \n",
      "Epoch 00048 | Loss 0.2601 | Accuracy 0.7440 \n",
      "Epoch 00049 | Loss 0.2427 | Accuracy 0.7360 \n",
      "Epoch 00050 | Loss 0.2422 | Accuracy 0.7360 \n",
      "Epoch 00051 | Loss 0.2265 | Accuracy 0.7360 \n",
      "Epoch 00052 | Loss 0.2293 | Accuracy 0.7360 \n",
      "Epoch 00053 | Loss 0.2331 | Accuracy 0.7360 \n",
      "Epoch 00054 | Loss 0.2277 | Accuracy 0.7360 \n",
      "Epoch 00055 | Loss 0.2259 | Accuracy 0.7360 \n",
      "Epoch 00056 | Loss 0.2249 | Accuracy 0.7440 \n",
      "Epoch 00057 | Loss 0.2301 | Accuracy 0.7440 \n",
      "Epoch 00058 | Loss 0.2134 | Accuracy 0.7440 \n",
      "Epoch 00059 | Loss 0.2267 | Accuracy 0.7440 \n",
      "Epoch 00060 | Loss 0.2226 | Accuracy 0.7440 \n",
      "Epoch 00061 | Loss 0.2033 | Accuracy 0.7440 \n",
      "Epoch 00062 | Loss 0.2140 | Accuracy 0.7440 \n",
      "Epoch 00063 | Loss 0.2133 | Accuracy 0.7360 \n",
      "Epoch 00064 | Loss 0.2236 | Accuracy 0.7360 \n",
      "Epoch 00065 | Loss 0.2078 | Accuracy 0.7360 \n",
      "Epoch 00066 | Loss 0.2307 | Accuracy 0.7360 \n",
      "Epoch 00067 | Loss 0.2209 | Accuracy 0.7360 \n",
      "Epoch 00068 | Loss 0.2017 | Accuracy 0.7360 \n",
      "Epoch 00069 | Loss 0.1991 | Accuracy 0.7360 \n",
      "Epoch 00070 | Loss 0.2129 | Accuracy 0.7360 \n",
      "Epoch 00071 | Loss 0.2041 | Accuracy 0.7360 \n",
      "Epoch 00072 | Loss 0.2255 | Accuracy 0.7440 \n",
      "Epoch 00073 | Loss 0.2006 | Accuracy 0.7440 \n",
      "Epoch 00074 | Loss 0.2092 | Accuracy 0.7440 \n",
      "Epoch 00075 | Loss 0.2022 | Accuracy 0.7440 \n",
      "Epoch 00076 | Loss 0.2096 | Accuracy 0.7440 \n",
      "Epoch 00077 | Loss 0.2076 | Accuracy 0.7440 \n",
      "Epoch 00078 | Loss 0.1971 | Accuracy 0.7360 \n",
      "Epoch 00079 | Loss 0.1951 | Accuracy 0.7360 \n",
      "Epoch 00080 | Loss 0.2019 | Accuracy 0.7360 \n",
      "Epoch 00081 | Loss 0.1850 | Accuracy 0.7360 \n",
      "Epoch 00082 | Loss 0.1982 | Accuracy 0.7360 \n",
      "Epoch 00083 | Loss 0.2021 | Accuracy 0.7360 \n",
      "Epoch 00084 | Loss 0.1860 | Accuracy 0.7360 \n",
      "Epoch 00085 | Loss 0.1944 | Accuracy 0.7360 \n",
      "Epoch 00086 | Loss 0.1745 | Accuracy 0.7360 \n",
      "Epoch 00087 | Loss 0.1905 | Accuracy 0.7360 \n",
      "Epoch 00088 | Loss 0.1875 | Accuracy 0.7360 \n",
      "Epoch 00089 | Loss 0.1818 | Accuracy 0.7360 \n",
      "Epoch 00090 | Loss 0.1867 | Accuracy 0.7440 \n",
      "Epoch 00091 | Loss 0.1885 | Accuracy 0.7440 \n",
      "Epoch 00092 | Loss 0.1864 | Accuracy 0.7440 \n",
      "Epoch 00093 | Loss 0.1894 | Accuracy 0.7440 \n",
      "Epoch 00094 | Loss 0.1892 | Accuracy 0.7440 \n",
      "Epoch 00095 | Loss 0.1892 | Accuracy 0.7440 \n",
      "Epoch 00096 | Loss 0.1860 | Accuracy 0.7360 \n",
      "Epoch 00097 | Loss 0.1721 | Accuracy 0.7360 \n",
      "Epoch 00098 | Loss 0.1855 | Accuracy 0.7360 \n",
      "Epoch 00099 | Loss 0.1840 | Accuracy 0.7360 \n",
      "Epoch 00100 | Loss 0.1780 | Accuracy 0.7360 \n",
      "Epoch 00101 | Loss 0.1826 | Accuracy 0.7440 \n",
      "Epoch 00102 | Loss 0.1807 | Accuracy 0.7440 \n",
      "Epoch 00103 | Loss 0.1770 | Accuracy 0.7440 \n",
      "Epoch 00104 | Loss 0.1695 | Accuracy 0.7440 \n",
      "Epoch 00105 | Loss 0.1731 | Accuracy 0.7440 \n",
      "Epoch 00106 | Loss 0.1922 | Accuracy 0.7440 \n",
      "Epoch 00107 | Loss 0.1731 | Accuracy 0.7440 \n",
      "Epoch 00108 | Loss 0.1837 | Accuracy 0.7440 \n",
      "Epoch 00109 | Loss 0.1733 | Accuracy 0.7440 \n",
      "Epoch 00110 | Loss 0.1810 | Accuracy 0.7440 \n",
      "Epoch 00111 | Loss 0.1763 | Accuracy 0.7440 \n",
      "Epoch 00112 | Loss 0.1680 | Accuracy 0.7440 \n",
      "Epoch 00113 | Loss 0.1746 | Accuracy 0.7440 \n",
      "Epoch 00114 | Loss 0.1714 | Accuracy 0.7440 \n",
      "Epoch 00115 | Loss 0.1787 | Accuracy 0.7440 \n",
      "Epoch 00116 | Loss 0.1665 | Accuracy 0.7360 \n",
      "Epoch 00117 | Loss 0.1753 | Accuracy 0.7360 \n",
      "Epoch 00118 | Loss 0.1705 | Accuracy 0.7360 \n",
      "Epoch 00119 | Loss 0.1661 | Accuracy 0.7360 \n",
      "Epoch 00120 | Loss 0.1699 | Accuracy 0.7360 \n",
      "Epoch 00121 | Loss 0.1726 | Accuracy 0.7360 \n",
      "Epoch 00122 | Loss 0.1661 | Accuracy 0.7360 \n",
      "Epoch 00123 | Loss 0.1684 | Accuracy 0.7440 \n",
      "Epoch 00124 | Loss 0.1738 | Accuracy 0.7440 \n",
      "Epoch 00125 | Loss 0.1739 | Accuracy 0.7440 \n",
      "Epoch 00126 | Loss 0.1653 | Accuracy 0.7440 \n",
      "Epoch 00127 | Loss 0.1588 | Accuracy 0.7360 \n",
      "Epoch 00128 | Loss 0.1730 | Accuracy 0.7360 \n",
      "Epoch 00129 | Loss 0.1643 | Accuracy 0.7360 \n",
      "Epoch 00130 | Loss 0.1658 | Accuracy 0.7360 \n",
      "Epoch 00131 | Loss 0.1585 | Accuracy 0.7360 \n",
      "Epoch 00132 | Loss 0.1643 | Accuracy 0.7360 \n",
      "Epoch 00133 | Loss 0.1649 | Accuracy 0.7360 \n",
      "Epoch 00134 | Loss 0.1504 | Accuracy 0.7360 \n",
      "Epoch 00135 | Loss 0.1619 | Accuracy 0.7360 \n",
      "Epoch 00136 | Loss 0.1592 | Accuracy 0.7360 \n",
      "Epoch 00137 | Loss 0.1706 | Accuracy 0.7360 \n",
      "Epoch 00138 | Loss 0.1669 | Accuracy 0.7360 \n",
      "Epoch 00139 | Loss 0.1645 | Accuracy 0.7440 \n",
      "Epoch 00140 | Loss 0.1611 | Accuracy 0.7440 \n",
      "Epoch 00141 | Loss 0.1613 | Accuracy 0.7440 \n",
      "Epoch 00142 | Loss 0.1577 | Accuracy 0.7440 \n",
      "Epoch 00143 | Loss 0.1576 | Accuracy 0.7440 \n",
      "Epoch 00144 | Loss 0.1548 | Accuracy 0.7440 \n",
      "Epoch 00145 | Loss 0.1506 | Accuracy 0.7360 \n",
      "Epoch 00146 | Loss 0.1641 | Accuracy 0.7360 \n",
      "Epoch 00147 | Loss 0.1530 | Accuracy 0.7360 \n",
      "Epoch 00148 | Loss 0.1517 | Accuracy 0.7360 \n",
      "Epoch 00149 | Loss 0.1462 | Accuracy 0.7440 \n",
      "Epoch 00150 | Loss 0.1576 | Accuracy 0.7440 \n",
      "Epoch 00151 | Loss 0.1707 | Accuracy 0.7440 \n",
      "Epoch 00152 | Loss 0.1616 | Accuracy 0.7440 \n",
      "Epoch 00153 | Loss 0.1522 | Accuracy 0.7440 \n",
      "Epoch 00154 | Loss 0.1500 | Accuracy 0.7440 \n",
      "Epoch 00155 | Loss 0.1516 | Accuracy 0.7360 \n",
      "Epoch 00156 | Loss 0.1552 | Accuracy 0.7360 \n",
      "Epoch 00157 | Loss 0.1560 | Accuracy 0.7360 \n",
      "Epoch 00158 | Loss 0.1463 | Accuracy 0.7440 \n",
      "Epoch 00159 | Loss 0.1387 | Accuracy 0.7440 \n",
      "Epoch 00160 | Loss 0.1564 | Accuracy 0.7360 \n",
      "Epoch 00161 | Loss 0.1566 | Accuracy 0.7360 \n",
      "Epoch 00162 | Loss 0.1493 | Accuracy 0.7360 \n",
      "Epoch 00163 | Loss 0.1565 | Accuracy 0.7360 \n",
      "Epoch 00164 | Loss 0.1583 | Accuracy 0.7360 \n",
      "Epoch 00165 | Loss 0.1563 | Accuracy 0.7440 \n",
      "Epoch 00166 | Loss 0.1457 | Accuracy 0.7440 \n",
      "Epoch 00167 | Loss 0.1526 | Accuracy 0.7440 \n",
      "Epoch 00168 | Loss 0.1565 | Accuracy 0.7440 \n",
      "Epoch 00169 | Loss 0.1557 | Accuracy 0.7440 \n",
      "Epoch 00170 | Loss 0.1573 | Accuracy 0.7440 \n",
      "Epoch 00171 | Loss 0.1450 | Accuracy 0.7440 \n",
      "Epoch 00172 | Loss 0.1544 | Accuracy 0.7360 \n",
      "Epoch 00173 | Loss 0.1471 | Accuracy 0.7360 \n",
      "Epoch 00174 | Loss 0.1587 | Accuracy 0.7360 \n",
      "Epoch 00175 | Loss 0.1429 | Accuracy 0.7360 \n",
      "Epoch 00176 | Loss 0.1532 | Accuracy 0.7360 \n",
      "Epoch 00177 | Loss 0.1487 | Accuracy 0.7360 \n",
      "Epoch 00178 | Loss 0.1519 | Accuracy 0.7360 \n",
      "Epoch 00179 | Loss 0.1524 | Accuracy 0.7440 \n",
      "Epoch 00180 | Loss 0.1483 | Accuracy 0.7360 \n",
      "Epoch 00181 | Loss 0.1438 | Accuracy 0.7360 \n",
      "Epoch 00182 | Loss 0.1594 | Accuracy 0.7360 \n",
      "Epoch 00183 | Loss 0.1371 | Accuracy 0.7360 \n",
      "Epoch 00184 | Loss 0.1532 | Accuracy 0.7360 \n",
      "Epoch 00185 | Loss 0.1485 | Accuracy 0.7360 \n",
      "Epoch 00186 | Loss 0.1338 | Accuracy 0.7360 \n",
      "Epoch 00187 | Loss 0.1422 | Accuracy 0.7360 \n",
      "Epoch 00188 | Loss 0.1433 | Accuracy 0.7360 \n",
      "Epoch 00189 | Loss 0.1334 | Accuracy 0.7360 \n",
      "Epoch 00190 | Loss 0.1544 | Accuracy 0.7360 \n",
      "Epoch 00191 | Loss 0.1435 | Accuracy 0.7360 \n",
      "Epoch 00192 | Loss 0.1417 | Accuracy 0.7360 \n",
      "Epoch 00193 | Loss 0.1483 | Accuracy 0.7440 \n",
      "Epoch 00194 | Loss 0.1593 | Accuracy 0.7440 \n",
      "Epoch 00195 | Loss 0.1416 | Accuracy 0.7440 \n",
      "Epoch 00196 | Loss 0.1384 | Accuracy 0.7360 \n",
      "Epoch 00197 | Loss 0.1525 | Accuracy 0.7360 \n",
      "Epoch 00198 | Loss 0.1457 | Accuracy 0.7360 \n",
      "Epoch 00199 | Loss 0.1483 | Accuracy 0.7360 \n",
      "Testing...\n",
      "Test accuracy 0.7520\n"
     ]
    }
   ],
   "source": [
    "# create GCN model\n",
    "in_size = features.shape[1]\n",
    "out_size = dataset.num_classes\n",
    "model = GCN(in_size, 16, out_size).to(device)\n",
    "\n",
    "# model training\n",
    "print(\"Training...\")\n",
    "train(graph, features, labels, masks, model)\n",
    "\n",
    "# test the model\n",
    "print(\"Testing...\")\n",
    "acc = evaluate(graph, features, labels, masks[2], model)\n",
    "print(\"Test accuracy {:.4f}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProGCL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
